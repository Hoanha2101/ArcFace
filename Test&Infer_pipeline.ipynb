{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, ReLU, Dropout, MaxPool2d, Sequential, Module\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test folder path \n",
    "\n",
    "test_path = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count - Check\n",
    "\n",
    "folder = os.listdir(test_path)\n",
    "count = 0\n",
    "print(len(folder))\n",
    "for i in folder:\n",
    "  path = os.path.join(test_path, i)\n",
    "  num_images = len(os.listdir(path))\n",
    "  count = count + num_images\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (112,112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone.model_irse import IR_SE_50\n",
    "BACKBONE = IR_SE_50(INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "input_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([int(112), int(112)]),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "\n",
    "def TEST(folder_path, model, transforms):\n",
    "\n",
    "    label_org = []\n",
    "    dir_org = []\n",
    "    label_test = []\n",
    "    dir_test_path = []\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each subfolder in the folder_path\n",
    "        for label_index, subfolder_name in enumerate(os.listdir(folder_path)):\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            image_files = os.listdir(subfolder_path)\n",
    "\n",
    "            for image_index, image_file in enumerate(image_files):\n",
    "\n",
    "                image_path = os.path.join(subfolder_path, image_file)\n",
    "\n",
    "                if \"ORG_EM\" in image_path:\n",
    "\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image = transforms(image)\n",
    "                    # Extract the embedding for the first image in the folder\n",
    "                    embedding = model(image.unsqueeze(0).to(\"cuda\"))\n",
    "                    dir_org.append(embedding)\n",
    "                    label_org.append(label_index)\n",
    "                else:\n",
    "                    # Store the path and label for other images\n",
    "                    dir_test_path.append(image_path)\n",
    "                    label_test.append(label_index)\n",
    "\n",
    "        predict_label = []\n",
    "\n",
    "        # Iterate over test images\n",
    "        for test_image_path in dir_test_path:\n",
    "\n",
    "            test_image = Image.open(test_image_path).convert('RGB')\n",
    "            test_image = transforms(test_image)\n",
    "\n",
    "            # Extract the embedding for the test image\n",
    "            test_embedding = model(test_image.unsqueeze(0).to(\"cuda\"))\n",
    "            similarities = []\n",
    "\n",
    "            # Calculate cosine similarity with each original embedding\n",
    "            for org_embedding in dir_org:\n",
    "\n",
    "                cosine_sim = cosine_similarity(org_embedding.cpu().detach().numpy(), test_embedding.cpu().detach().numpy())\n",
    "                similarities.append(cosine_sim[0][0])\n",
    "\n",
    "            max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "            predict_label.append(label_org[max_similarity_index])\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(predict_label, label_test)\n",
    "        print(f'----Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 111, 10):\n",
    "  print(f'model {i}:')\n",
    "  model = torch.load(f\"checkpoint/ISE_{i}.pth\").to(\"cuda\")\n",
    "  TEST(test_path, model, input_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
